# День 27: Локальная LLM (Встроенный чат)

Автономное консольное чат-приложение, которое "носит свой мозг с собой". Запускает GGUF модели локально, используя JVM-биндинги для llama.cpp.

## Возможности
- **100% Оффлайн**: Без Ollama, без API ключей, интернет не требуется во время работы.
- **Встроенный Инференс**: Использует библиотеку `de.kherud:llama` для запуска моделей прямо в процессе приложения.
- **Интерфейс REPL**: Интерактивный консольный чат с поддержкой команд.
- **Настраиваемый Системный Промпт**: Изменяйте персону ИИ на лету.

## Требования
- JDK 17 или выше.
- LLM модель в формате GGUF (например, Phi-3, Llama-3, Mistral).

## Установка

1. **Скачайте модель**
   Вам нужен файл модели в формате GGUF. Рекомендуются небольшие модели (<= 3GB) для инференса на CPU.
   *   **Рекомендуемая**: [Phi-3 Mini 4k Instruct (Q4)](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf) (~2.4GB)
   *   Альтернатива: [TinyLlama 1.1B](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) (~600MB)

2. **Разместите модель**
   *   Переименуйте скачанный файл в `model.gguf`.
   *   Положите его в **корневую директорию** проекта.

## Запуск

```bash
./gradlew run
```

## Команды
*   `/system <text>` - Обновить системный промпт (сбрасывает контекст разговора).
*   `/exit` - Выйти из приложения.

## Устранение неполадок
*   **"Model not found"**: Убедитесь, что файл `model.gguf` находится в корне проекта (там же, где `build.gradle.kts`).
*   **Медленная работа**: По умолчанию работает на CPU. Производительность зависит от вашего железа и размера модели (количество параметров/квантование).

## Стек технологий
*   Kotlin JVM
*   [java-llama.cpp](https://github.com/kherud/java-llama.cpp) (v4.2.0)
